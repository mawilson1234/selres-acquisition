# tuner_utils.py
#
# utility functions for tuner.py
import re
import torch
import logging
import itertools

import numpy as np
import pandas as pd

from math import sqrt
from copy import deepcopy
from typing import *
from functools import partial
from omegaconf import OmegaConf, DictConfig, ListConfig
from scipy.stats import pearsonr

log = logging.getLogger(__name__)

# grammatical functions in order of prominence for sorting things
GF_ORDER = [
	'[subj]', 
	'[obj]', 
	'[2obj]', 
	'[iobj]', 
	'[obl]',
	'[pobj]', 
	'[adj]'
]

# short useful functions
def flatten(l: List) -> List:
	'''
	Flatten a list of lists or np.ndarrays without breaking strings into characters
	adapted from https://stackoverflow.com/questions/5286541/how-can-i-flatten-lists-without-splitting-strings
	
		params:
			l (list): a list to flatten
		
		returns:
			l (list): the list, flattened into 1 dimension
	'''
	l = apply_to_all_of_type(l, ListConfig, OmegaConf.to_container)
	if l is not None:
		return [k for j in ([i] if not isinstance(i,(list,tuple,np.ndarray)) else flatten(i) for i in l) for k in j]

def listify(l: 'any') -> List:
	'''
	If something isn't a list, return it as a list. If it is, return it without changing it.
		
		params:
			l (any)	: an object that we want to ensure is a list
		
		returns:
			l (list): the object as/in a list
	'''
	if isinstance(l,list):
		return l
	
	if isinstance(l,ListConfig):
		return OmegaConf.to_container(l)
	
	if isinstance(l,tuple):
		return list(l)
	
	if isinstance(l,(np.ndarray,pd.Series)):
		return l.tolist()
	
	return [l]

def unlistify(l: 'any') -> 'any':
	'''
	Safely unlists one-element lists/dicts
	
		params:
			l (any): an iterable object with 1 or more elements inside it
		
		returns:
			l (any): if iterable has only one object inside it, return that object.
					 otherwise, return the original object
	'''
	if isinstance(l,dict):
		return l[list(l.keys())[0]] if len(l) == 1 else l
	
	return l[0] if isinstance(l,list) or isinstance(l,tuple) and len(l) == 1 else l

def none(iterator: 'Iterator') -> bool:
	'''
	Convenience function wrapping not any/all not
	
		params:
			iterator: an iterator expression that generates booleans
		
		returns;
			bool 	: True if all expressions generated by iterator are False,
					  otherwise False
	'''
	return not any(iterator)

def sem(x: Union[List,np.ndarray,torch.Tensor]) -> float:
	'''
	Calculate the standard error of the mean for a list of numbers
	
		params:
			x (list) 		: a list of numbers for which to calculate the standard error of the mean
		
		returns:
			sem_x (float)	: the standard error of the mean of x
	'''
	namespace = torch if isinstance(x,torch.Tensor) else np
	return namespace.std(x)/sqrt(len(x))

def apply_to_all_of_type(
	data: 'any', 
	t: Type,
	fun: Callable,
	*args: Tuple, 
	**kwargs: Dict
) -> 'any':
	'''
	Apply a function to recursively to all elements in an iterable that match the specified type
		
		params:
			data (any)		: an object to recursively apply a function to
			t (type)		: the type of object within data to apply the function to
			fun (Callable)	: the function to apply to any values within data of type t
			*args (tuple)	: passed to fun
			**kwargs (dict)	: passed to fun
		
		returns:
			data (any)		: the data object with fun applied to everything in data matching type t
	'''
	if isinstance(data,(DictConfig,ListConfig)):
		# we need the primitive versions of these so we can modify them
		data = OmegaConf.to_container(data)
	
	data = deepcopy(data)
	
	if isinstance(data,t):
		returns = fun(data, *args, **kwargs)
	elif isinstance(data,dict):
		returns = {apply_to_all_of_type(k, t, fun, *args, **kwargs): apply_to_all_of_type(v, t, fun, *args, **kwargs) for k, v in data.items()}
	elif isinstance(data,(list,tuple,set)):
		returns = type(data)(apply_to_all_of_type(i, t, fun, *args, **kwargs) for i in data)
	elif isinstance(data,(torch.Tensor,pd.Series)):
		returns = type(data)([apply_to_all_of_type(i, t, fun, *args, **kwargs) for i in data])
	elif isinstance(data,np.ndarray):
		returns = np.array([apply_to_all_of_type(i, t, fun, *args, **kwargs) for i in data])
	else:
		returns = data
	
	if isinstance(data,(pd.Series,np.ndarray)):
		return returns if returns.any() or returns.size == 1 and returns[0] == 0 else None
	else:
		return returns

def crawler(t: 'type') -> Callable:
	'''
	Creates functions that crawl through a nested data structure 
	and apply a transformation to a single data type.
		
		params:
			t (type)		: the type to apply the function to
	'''
	return lambda fun: \
		lambda data, *args, **kwargs: \
			apply_to_all_of_type(data=data, t=t, fun=fun, *args, **kwargs)

# summary and file related
def get_file_prefix(summary: pd.DataFrame) -> str:
	'''
	Creates an appropriate file prefix for saving various output files
	
		params:
			summary (pd.DataFrame)	: a summary dataframe containing information about the experiment's configuration
		
		returns:
			file_prefix (str)		: a string containing the dataset and epoch information to prefix to saved files
	'''
	file_prefix = summary.data.unique()[0]
	
	return file_prefix

def get_sentence_label(data: pd.DataFrame) -> str:
	'''
	Replaces masked tokens in sentences with display values for plot/accuracy labels
	
		params:
			data (pd.DataFrame)	: a dataframe containing sentence examples
		
		returns:
			sentence_ex (str)	: a sentence example from the data frame with mask tokens replaced with display values
	'''
	first_rows = data[data.sentence == data.loc[0].sentence][['ratio_name', 'position_ratio_name', 'sentence']].drop_duplicates().reset_index(drop=True)
	position_map = {}
	for row in first_rows.index:
		position_map.update({gf: position for gf, position in tuple(zip(first_rows.loc[row].ratio_name.split('/'), [int(p) for p in first_rows.loc[row].position_ratio_name.replace('position ', '').split('/')]))})
	
	position_map = dict(sorted(position_map.items(), key=lambda item: item[1]))
	sentence_ex = first_rows.sentence[0]
	for gf in position_map:
		# this is bit hacky, but it's to ensure it'll work in cases with multiple models' data
		# where we can't rely on the mask token for each model being available.
		sentence_ex = re.sub(r'^(.*?)(\[MASK\]|\<mask\>)', f'\\1{gf}', sentence_ex)
	
	return sentence_ex

def get_data_for_pairwise_comparisons(
	summary: pd.DataFrame,
	eval_cfg: DictConfig = None, 
	cossims: bool = False, 
	diffs: bool = False
) -> List:
	'''
	Get data for performing pairwise comparisons for plots and accuracy metrics
	
		params:
			summary (pd.DataFrame)	: a summary containing results information
			eval_cfg (DictConfig)	: a configuration for the evaluation being performed
			cossims (bool)			: are you getting pairwise data for cosine similarities?
			diffs (bool)			: are you getting pairwise data for a differences measure?
		
		returns:
			summary (pd.DataFrame)	: the summary formatted for comparisons
			colnames (tuple[str])	: the column names corresponding to the summary metrics
			pairs (list)			: a list of pairs to compare data in colnames for
	'''
	
	def get_pairs(
		summary: pd.DataFrame, 
		eval_cfg: DictConfig = None, 
		cossims: bool = False
	) -> List[Tuple[str]]:
		'''
		Get pairs of values for comparison groups
		
			params:
				summary (pd.DataFrame)		: a summary containing results information
				eval_cfg (DictConfig)		: a configuration for the type of experiment being performed
				cossims (bool)				: are you getting pairs for cosine similarity measures?
			
			returns:
				pairs (list(tuple(str)))	: a list of pairs of strings to be used for comparisons
		'''
		if not cossims:
			# Get each unique pair of sentence types so we can create a separate plot for each pair
			types = summary.sentence_type.unique()
			types = sorted(types, key=lambda s_t: eval_cfg.data.sentence_types.index(s_t))
		else:
			types = summary.predicted_arg.unique()
		
		pairs = [pair for pair in list(itertools.combinations(types, 2)) if not pair[0] == pair[1]]
		
		if not cossims:
			# Sort so that the trained cases are first
			reference_sentence_type = multiplator(summary.reference_sentence_type)
			pairs = [sorted(pair, key=lambda x: str(-int(x == reference_sentence_type)) + x) for pair in pairs]
			
			# Filter to only cases including the reference sentence type for ease of interpretation
			pairs = [(s1, s2) for s1, s2 in pairs if s1 == reference_sentence_type] if reference_sentence_type != 'none' else pairs
		else:
			pairs = list(set(tuple(sorted(pair)) for pair in pairs))
		
		return pairs
	
	def format_summary_for_comparisons(
		summary: pd.DataFrame, 
		exp_type: str = None, 
		cossims: bool = False, 
		diffs: bool = False
	) -> Tuple[pd.DataFrame,Tuple[str]]:
		'''
		Formats a summary for comparisons by sorting for grammatical function prominence and returning the correct metrics colnames
		
			params:
				summary (pd.DataFrame)	: a summary to format
				exp_type (str)			: the type of experiment being evaluated
				cossims (bool)			: are you making comparisons for cosine similarities?
				diffs (bool)			: are you making comparisons for a difference measure?
			
			returns:
				summary (pd.DataFrame)	: the summary sorted according to grammatical function prominence (if applicable)
				colnames (tuple)		: the colnames in summary containing the evaluation metrics
		'''
		summary = summary.copy()
				
		if exp_type == 'newverb' and not cossims:
			# Sort by grammatical function prominence for newverb exps (we do this because 'subj' alphabetically follows 'obj'), but it's more natural for it to go first
			summary['ratio_order'] 	= [GF_ORDER.index(gf_ratio_name.split('/')[0]) for gf_ratio_name in summary.ratio_name]
			summary 				= summary.sort_values(['model_id', 'ratio_order'])
			summary 				= summary.drop('ratio_order', axis=1)
			summary['ratio_name'] 	= [re.sub(r'\[|\]', '', ratio_name) for ratio_name in summary.ratio_name]
		
		colnames 	= get_eval_metrics_colnames(exp_type, cossims, diffs)
		metric 		= colnames[-2]
		
		if summary.model_id.unique().size > 1:
			colnames[-2] = f'{metric}_mean'
		else:
			summary[f'{metric}_sem'] = 0
		
		return summary, colnames
	
	def get_eval_metrics_colnames(
		exp_type: str, 
		cossims: bool = False, 
		diffs: bool = False
	) -> List[str]:
		'''
		Get the correct metric for the type of experiment being performed
		
			params:
				exp_type (str)	: the type of experiment being performed
				cossims (bool)	: are you getting colnames for cosine similarity measures?
				diffs (bool)	: are you getting colnames for a differences measure?
			
			returns:
				colnames (list)	: a list of strings containing the correct column name of the metric and sem measures for the given experiment type/data
		'''
		if not cossims:
			metric = 'odds_ratio' if exp_type == 'newarg' or not diffs else 'odds_ratio_pre_post_difference' if exp_type == 'newverb' else None
		elif cossims:
			metric = 'cossim'
		
		semmetric = f'{metric}_sem'
		
		return [metric, semmetric]
	
	exp_type 			= eval_cfg.data.exp_type if eval_cfg is not None else None
	summary, colnames 	= format_summary_for_comparisons(summary, exp_type, cossims=cossims, diffs=diffs)
	pairs 				= get_pairs(summary, eval_cfg, cossims)
	
	return summary, tuple(colnames), pairs

def transfer_hyperparameters_to_df(
	source: pd.DataFrame, 
	target: pd.DataFrame
) -> pd.DataFrame:
	'''
	Transfers hyperparameter information from one dataframe to another
	
		params:
			source (pd.DataFrame) : a dataframe containing hyperparameter information
			target (pd.DataFrame) : a dataframe not containing hyperparameter information
		
		returns:
			target (pd.DataFrame) : target with the hyperparameters information from source added
	'''
	hp_cols = [
		c for c in source.columns if not c in [
			'odds_ratio', 'odds_ratio_mean', 'odds_ratio_sem', 'ratio_name', 'position_ratio_name',
			'role_position', 'token_type', 'token_id', 'token', 
			'sentence', 'sentence_type', 'sentence_num', 'odds_ratio_pre_post_difference',
			'odds_ratio_pre_post_difference_mean', 'odds_ratio_pre_post_difference_sem',
			'both_correct', 'both_incorrect', 'gen_correct', 'gen_incorrect', 
			'ref_correct', 'ref_incorrect', 'ref_correct_gen_incorrect',
			'ref_incorrect_gen_correct', 'specificity', 'specificity_se',
			'gen_given_ref', 's1', 's2', 's1_ex', 's2_ex', 'arg_type',
			'predicted_arg', 'predicted_role',
		] 
		and not c.endswith('_ref') and not c.endswith('_gen')
	]
	
	for c in hp_cols:
		target[c] = multiplator(source[c])
	
	return target

def get_single_pair_data(
	summary: pd.DataFrame, 
	pair: Tuple[str], 
	group: str, 
	pair_col: str = 'sentence_type'
) -> Tuple:
	'''
	Get data for a single pairwise comparison
	
		params:
			summary (pd.DataFrame)			: a dataframe containing experiment results
			pair (tuple(str))				: a tuple with labels indicating the pair to get data for
			group (str)						: a column name indicating groups to compare data for 
											  (comparison data must include information about all groups)
			pair_col (str)					: which column contains the labels in pair?
		
		returns:
			x_data, y_data (pd.DataFrame)	: information from summary where pair_col == pair[0] and pair_col == pair[1]
											  and all groups in the group column occur in both x and y
	'''
	# do some sorting
	summary = summary[summary[pair_col].isin(pair)].reset_index(drop=True)
	
	# we need 'kind='stable'' because otherwise other columns get into weird orders
	summary = summary.sort_values(group, kind='stable').reset_index(drop=True)
	
	if 'ratio_name' in summary and any(re.sub(r'\[|\]', '', gf) in ratio_name for gf in GF_ORDER for ratio_name in summary.ratio_name):
		# Sort by grammatical function prominence for newverb exps (we do this because 'subj' alphabetically follows 'obj'), but it's more natural for it to go first
		summary['ratio_order'] 	= [GF_ORDER.index(f'[{gf_ratio_name.split("/")[0]}]') for gf_ratio_name in summary.ratio_name]
		summary 				= summary.sort_values(['model_id', 'ratio_order'])
		summary 				= summary.drop('ratio_order', axis=1)
		summary.ratio_name		= [re.sub(r'\[|\]', '', ratio_name) for ratio_name in summary.ratio_name]
	
	x_data = summary[summary[pair_col] == pair[0]].reset_index(drop=True)
	y_data = summary[summary[pair_col] == pair[1]].reset_index(drop=True)
	
	# Filter data to groups that only exist in both sets
	common_groups = set(x_data[group]).intersection(y_data[group])
	x_data = x_data[x_data[group].isin(common_groups)].reset_index(drop=True)
	y_data = y_data[y_data[group].isin(common_groups)].reset_index(drop=True)
	
	return x_data, y_data

def get_accuracy_measures(
	refs: pd.DataFrame, 
	gens: pd.DataFrame, 
	colname: str
) -> Dict:
	'''
	Get prediction accuracy measures
	
		params:
			refs (pd.DataFrame)	: a dataframe containing measures from the reference group
			gens (pd.DataFrame)	: a dataframe containing measures from the generalization group
			colname (str)		: the name of the column containing the values to compare
		
		returns:
			dict 				: a dictionary containing accuracy information about refs and gens
								  where accuracy is computed based on colname > 0 (intended for use with odds ratios)
	'''
	refs 						= refs.copy().reset_index(drop=True)
	gens 						= gens.copy().reset_index(drop=True)
	
	# need to convert to float for scipy stats
	# we could do this with torch, but given the output is harder to work with it's not really worth it
	r, p_r						= pearsonr([float(t) for t in refs[colname]], [float(t) for t in gens[colname]])
	
	refs_correct 				= refs[colname] > 0
	gens_correct 				= gens[colname] > 0
	num_points 					= len(refs.index)
	
	gen_given_ref 				= sum(gens_correct.loc[np.where(refs_correct)])/len(refs_correct.loc[np.where(refs_correct)]) * 100 if not refs_correct.loc[np.where(refs_correct)].empty else np.nan
	both_correct 				= sum(refs_correct * gens_correct)/num_points * 100
	both_incorrect				= sum(-refs_correct * -gens_correct)/num_points * 100
	ref_correct 				= sum(refs_correct)/num_points * 100
	ref_incorrect 				= 100. - ref_correct
	gen_correct 				= sum(gens_correct)/num_points * 100
	gen_incorrect 				= 100. - gen_correct
	ref_correct_gen_incorrect 	= sum( refs_correct * -gens_correct)/num_points * 100
	ref_incorrect_gen_correct 	= sum(-refs_correct *  gens_correct)/num_points * 100
	
	sq_err						= (gens[colname] - refs[colname])**2
	specificity 				= np.mean(sq_err)
	specificity_se 				= np.std(sq_err)/sqrt(num_points)
	
	return {
		'gen_given_ref'				: gen_given_ref,
		'both_correct'				: both_correct,
		'both_incorrect'			: both_incorrect,
		'ref_correct'				: ref_correct,
		'ref_incorrect'				: ref_incorrect,
		'gen_correct'				: gen_correct,
		'gen_incorrect'				: gen_incorrect,
		'ref_correct_gen_incorrect'	: ref_correct_gen_incorrect,
		'ref_incorrect_gen_correct'	: ref_incorrect_gen_correct,
		'specificity_(MSE)'			: specificity,
		'specificity_se'			: specificity_se,
		'r'		 					: r,
		'p_r'						: p_r,		
	}

def get_odds_ratios_accuracies(
	summary: pd.DataFrame, 
	eval_cfg: DictConfig, 
	get_diffs_accuracies: bool = False
) -> pd.DataFrame:
	'''
	Create a dataframe containing information about odds ratios accuracies
	
		params:
			summary (pd.DataFrame)			: a summary containing odds ratios results
			eval_cfg (DictConfig)			: the evaluation's parameters
			get_diffs_accuracies (bool)		: are you getting accuracy for improvements in odds ratios over the experiment?
		
		returns:
			acc (pd.DataFrame)				: a dataframe containing accuracy information
	'''
	def update_acc(acc: List[Dict], refs: pd.DataFrame, gens: pd.DataFrame, colname: str, **addl_columns) -> None:
		'''
		Update the accuracy list
		
			params:
				acc (List)				: the list of accuracy measures to update
				refs (pd.DataFrame)		: the data from the reference group
				gens (pd.DataFrame)		: the data from the generalization group
				colname (str)			: the name of the column containing the measure to use to compute accuracy
				**addl_columns (dict)	: additional information to include in each added dictionary
		'''
		acc_data = get_accuracy_measures(refs=refs, gens=gens, colname=colname)
		
		cols = {
			**addl_columns,
			**acc_data,
			'token'		: multiplator(refs.token, multstr='any'),
			'token_id'	: multiplator(refs.token_id),
		}
		
		if 'token_type' in refs.columns:
			cols = {**cols, 'token_type': multiplator(refs.token_type)}
		
		acc.append(cols)
	
	summary, (odds_ratio, odds_ratio_sem), paired_sentence_types = get_data_for_pairwise_comparisons(summary, eval_cfg=eval_cfg, diffs=get_diffs_accuracies)
	
	acc = []
	
	for pair in paired_sentence_types:
		x_data, y_data = get_single_pair_data(summary, pair, 'ratio_name')
		
		s1_ex = get_sentence_label(x_data)
		s2_ex = get_sentence_label(y_data)
		
		common_args = {
			 'acc'						: acc,
			 'colname'					: odds_ratio,
			 's1'						: pair[0], 
			 's2'						: pair[1], 
			 's1_ex'					: s1_ex, 
			 's2_ex'					: s2_ex,
			 'ratio_name'				: multiplator(x_data.ratio_name),
			 'predicted_arg'			: multiplator(x_data.ratio_name.str.split('/')[0], multstr='any'),
			f'position_ratio_name_ref'	: multiplator(x_data.position_ratio_name),
			f'position_ratio_name_gen'	: multiplator(y_data.position_ratio_name),
		}
		
		if eval_cfg.data.exp_type == 'newverb':
			common_args.update({'args_group': multiplator(x_data.args_group, multstr='any')})
		elif eval_cfg.data.exp_type == 'newarg':
			common_args.update({'predicted_role': multiplator(x_data.role_position, multstr='any')})
		
		update_acc(refs=x_data, gens=y_data, **common_args)
		
		if x_data.ratio_name.unique().size > 1:
			for name, x_group in x_data.groupby('ratio_name'):
				y_group = y_data[y_data.ratio_name == name]
				
				common_args.update({
					 'ratio_name'				: name,
					 'predicted_arg'			: name.split('/')[0],
					f'position_ratio_name_ref'	: multiplator(x_group.position_ratio_name),
					f'position_ratio_name_gen'	: multiplator(y_group.position_ratio_name),
				})
				
				if eval_cfg.data.exp_type == 'newarg':
					common_args.update({'predicted_role': x_group.role_position.unique()[0].split()[0]})
				
				update_acc(refs=x_group, gens=y_group, **common_args)
				
				if x_group.token.unique().size > 1:
					for token, x_token_group in x_group.groupby('token'):
						y_token_group = y_data[y_data.token == token]
						update_acc(refs=x_token_group, gens=y_token_group, **common_args)
	
	acc = pd.DataFrame(acc)
	
	return acc

def move_cols(
	df: pd.DataFrame, 
	cols_to_move: List[str] = [], 
	ref_col: str = None, 
	position: str = 'after'
) -> pd.DataFrame:
    '''
    Reorders columns in a dataframe by name and (optionally) relative position to a reference column.
    From https://towardsdatascience.com/reordering-pandas-dataframe-columns-thumbs-down-on-standard-solutions-1ff0bc2941d5
    
    	params:
    		df (pd.DataFrame)	: a dataframe
    		cols_to_move (list)	: a list of column names to be moved
    		ref_col (str)		: the column relative to which the cols to move should be positioned
    		place (str)			: one of 'before', 'after'. whether to place the cols to move before or after the ref col
    	
    	returns:
    		df (pd.DataFrame)	: the dataframe with the columns reordered
    '''
    cols 		= list(df.columns)
    cols_to_move = listify(cols_to_move)
    
    # if the ref col is not provide or is not in the columns, move columns to the front
    index 		= cols.index(ref_col) if ref_col is not None and ref_col in cols else 0
    position 	= 'before' if index == 0 else position
           
    if position == 'after':
        seg1 = cols[:index+1]
        seg2 = cols_to_move
    else:
        seg1 = cols[:index]
        seg2 = cols_to_move + ([ref_col] if ref_col else [])
    
    seg1 = [i for i in seg1 if i not in seg2]
    seg3 = [i for i in cols if i not in seg1 + seg2]
    
    return(df[seg1 + seg2 + seg3])

@crawler(str)
def format_data_for_tokenizer(data: str, mask_token: str) -> str:
	'''
	Format a string for use with a tokenizer
	Recursor means that this applies recursively to any nested data structure, formatting all tokens,
	and outputs data in the same shape as the input
	
		params:
			data (str)			: the data to format for use with a tokenizer
			mask_token (str)	: the tokenizer's mask token
		
		returns:
			the data formatted for use with the tokenizer in string_id
	'''
	return data.lower().replace(mask_token.lower(), mask_token)

def format_checkpoint(s: str) -> str:
	'''Formats a string with a multiBERTs checkpoint for display.'''
	s = s.replace('google/', '').replace('-seed', '')
	s = re.sub('-step_(.*)k', lambda m: f'_{m.group(1).zfill(4)}', s)
	return s